{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26941bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:42:32.496331Z",
     "iopub.status.busy": "2025-11-09T11:42:32.496000Z",
     "iopub.status.idle": "2025-11-09T11:43:04.074609Z",
     "shell.execute_reply": "2025-11-09T11:43:04.073582Z"
    },
    "papermill": {
     "duration": 31.585225,
     "end_time": "2025-11-09T11:43:04.076367",
     "exception": false,
     "start_time": "2025-11-09T11:42:32.491142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 11:42:39.472535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762688559.769988      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762688559.854724      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "# Brain MRI Image Anomaly Detection Pipeline\n",
    "# Detects brain tumor anomalies using feature extraction and multiple anomaly detection methods\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import zipfile\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest, AdaBoostClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, roc_auc_score, roc_curve)\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras import layers, models, losses, optimizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    KERAS_AVAILABLE = True\n",
    "except:\n",
    "    print(\"TensorFlow/Keras not available. Deep learning methods will be skipped.\")\n",
    "    KERAS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d69540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.085418Z",
     "iopub.status.busy": "2025-11-09T11:43:04.084260Z",
     "iopub.status.idle": "2025-11-09T11:43:04.100307Z",
     "shell.execute_reply": "2025-11-09T11:43:04.099464Z"
    },
    "papermill": {
     "duration": 0.021926,
     "end_time": "2025-11-09T11:43:04.101871",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.079945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 1: BRAIN MRI DATASET LOADER\n",
    "\n",
    "class BrainMRIDatasetLoader:\n",
    "    \"\"\"Load brain MRI dataset from Kaggle\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_root_path):\n",
    "        \"\"\"Initialize loader for brain MRI dataset\"\"\"\n",
    "        self.dataset_root = Path(dataset_root_path)\n",
    "        self.yes_tumor_dir = self.dataset_root / \"yes\"\n",
    "        self.no_tumor_dir = self.dataset_root / \"no\"\n",
    "        \n",
    "        self._validate_structure()\n",
    "    \n",
    "    def _validate_structure(self):\n",
    "        \"\"\"Validate that required directories exist\"\"\"\n",
    "        required_dirs = [self.yes_tumor_dir, self.no_tumor_dir]\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            if not dir_path.exists():\n",
    "                raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "        \n",
    "        print(f\"✓ Dataset structure validated\")\n",
    "        print(f\"  Tumor images: {self.yes_tumor_dir}\")\n",
    "        print(f\"  No tumor images: {self.no_tumor_dir}\")\n",
    "    \n",
    "    def load_tumor_images(self):\n",
    "        \"\"\"Load all brain MRI images with tumors\"\"\"\n",
    "        print(\"\\nLoading tumor images...\")\n",
    "        images = []\n",
    "        paths = []\n",
    "        \n",
    "        image_files = sorted(list(self.yes_tumor_dir.glob(\"*.jpg\"))) + \\\n",
    "                     sorted(list(self.yes_tumor_dir.glob(\"*.jpeg\"))) + \\\n",
    "                     sorted(list(self.yes_tumor_dir.glob(\"*.png\")))\n",
    "        \n",
    "        for idx, image_file in enumerate(image_files):\n",
    "            if (idx + 1) % 50 == 0 or idx == 0:\n",
    "                print(f\"  Loading: {idx + 1}/{len(image_files)}\")\n",
    "            \n",
    "            try:\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (256, 256))\n",
    "                    images.append(img)\n",
    "                    paths.append(str(image_file))\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {image_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(images)} tumor images\")\n",
    "        return images, paths\n",
    "    \n",
    "    def load_no_tumor_images(self):\n",
    "        \"\"\"Load all brain MRI images without tumors\"\"\"\n",
    "        print(\"\\nLoading non-tumor images...\")\n",
    "        images = []\n",
    "        paths = []\n",
    "        \n",
    "        image_files = sorted(list(self.no_tumor_dir.glob(\"*.jpg\"))) + \\\n",
    "                     sorted(list(self.no_tumor_dir.glob(\"*.jpeg\"))) + \\\n",
    "                     sorted(list(self.no_tumor_dir.glob(\"*.png\")))\n",
    "        \n",
    "        for idx, image_file in enumerate(image_files):\n",
    "            if (idx + 1) % 50 == 0 or idx == 0:\n",
    "                print(f\"  Loading: {idx + 1}/{len(image_files)}\")\n",
    "            \n",
    "            try:\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (256, 256))\n",
    "                    images.append(img)\n",
    "                    paths.append(str(image_file))\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {image_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(images)} non-tumor images\")\n",
    "        return images, paths\n",
    "    \n",
    "    def get_dataset_summary(self):\n",
    "        \"\"\"Print dataset summary\"\"\"\n",
    "        tumor_count = len(list(self.yes_tumor_dir.glob(\"*.jpg\"))) + \\\n",
    "                     len(list(self.yes_tumor_dir.glob(\"*.jpeg\"))) + \\\n",
    "                     len(list(self.yes_tumor_dir.glob(\"*.png\")))\n",
    "        \n",
    "        no_tumor_count = len(list(self.no_tumor_dir.glob(\"*.jpg\"))) + \\\n",
    "                        len(list(self.no_tumor_dir.glob(\"*.jpeg\"))) + \\\n",
    "                        len(list(self.no_tumor_dir.glob(\"*.png\")))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BRAIN MRI DATASET SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Tumor images: {tumor_count}\")\n",
    "        print(f\"Non-tumor images: {no_tumor_count}\")\n",
    "        print(f\"Total images: {tumor_count + no_tumor_count}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'tumor': tumor_count,\n",
    "            'no_tumor': no_tumor_count,\n",
    "            'total': tumor_count + no_tumor_count\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74088c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.109924Z",
     "iopub.status.busy": "2025-11-09T11:43:04.109168Z",
     "iopub.status.idle": "2025-11-09T11:43:04.126908Z",
     "shell.execute_reply": "2025-11-09T11:43:04.126120Z"
    },
    "papermill": {
     "duration": 0.023295,
     "end_time": "2025-11-09T11:43:04.128260",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.104965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 2: FEATURE EXTRACTION\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    \"\"\"Extract multiple types of features from brain MRI images\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=(256, 256)):\n",
    "        self.image_size = image_size\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def edge_detection(self, img):\n",
    "        \"\"\"Extract edge features using Canny edge detection\"\"\"\n",
    "        edges = cv2.Canny(img, 50, 150)\n",
    "        edge_features = np.array([\n",
    "            np.sum(edges) / edges.size,\n",
    "            np.std(edges),\n",
    "            np.count_nonzero(edges) / edges.size,\n",
    "        ])\n",
    "        return edge_features, edges\n",
    "    \n",
    "    def corner_detection_harris(self, img):\n",
    "        \"\"\"Extract corner features using Harris corner detection\"\"\"\n",
    "        corners = cv2.cornerHarris(img, 2, 3, 0.04)\n",
    "        corners = cv2.dilate(corners, None)\n",
    "        \n",
    "        corner_features = np.array([\n",
    "            np.sum(corners) / corners.size,\n",
    "            np.max(corners),\n",
    "            np.count_nonzero(corners > 0.01 * corners.max()) / corners.size,\n",
    "            np.std(corners),\n",
    "        ])\n",
    "        return corner_features, corners\n",
    "    \n",
    "    def blob_detection(self, img):\n",
    "        \"\"\"Extract blob features using SimpleBlobDetector\"\"\"\n",
    "        params = cv2.SimpleBlobDetector_Params()\n",
    "        params.filterByArea = True\n",
    "        params.minArea = 10\n",
    "        params.maxArea = 5000\n",
    "        params.filterByCircularity = True\n",
    "        params.minCircularity = 0.1\n",
    "        detector = cv2.SimpleBlobDetector_create(params)\n",
    "        \n",
    "        keypoints = detector.detect(img)\n",
    "        \n",
    "        blob_features = np.array([\n",
    "            len(keypoints),\n",
    "            np.mean([kp.size for kp in keypoints]) if keypoints else 0,\n",
    "            np.std([kp.size for kp in keypoints]) if len(keypoints) > 1 else 0,\n",
    "        ])\n",
    "        return blob_features, keypoints\n",
    "    \n",
    "    def ridge_detection(self, img):\n",
    "        \"\"\"Extract ridge features using Sobel derivatives\"\"\"\n",
    "        sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        \n",
    "        ridge_strength = np.sqrt(sobelx**2 + sobely**2)\n",
    "        \n",
    "        ridge_features = np.array([\n",
    "            np.mean(ridge_strength),\n",
    "            np.std(ridge_strength),\n",
    "            np.percentile(ridge_strength, 95),\n",
    "        ])\n",
    "        return ridge_features, ridge_strength\n",
    "    \n",
    "    def sift_features(self, img):\n",
    "        \"\"\"Extract SIFT keypoints and descriptors\"\"\"\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        \n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            sift_features = np.zeros(128)  # Fixed size\n",
    "        else:\n",
    "            # Use mean of descriptors for fixed-size output\n",
    "            sift_features = np.mean(descriptors, axis=0)\n",
    "        \n",
    "        return sift_features, keypoints, descriptors\n",
    "    \n",
    "    def surf_features(self, img):\n",
    "        \"\"\"Extract SURF keypoints and descriptors\"\"\"\n",
    "        try:\n",
    "            surf = cv2.xfeatures2d.SURF_create(400)\n",
    "            keypoints, descriptors = surf.detectAndCompute(img, None)\n",
    "            \n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                surf_features = np.zeros(64)  # Fixed size\n",
    "            else:\n",
    "                surf_features = np.mean(descriptors, axis=0)\n",
    "        except:\n",
    "            surf_features = np.zeros(64)\n",
    "            keypoints = []\n",
    "            descriptors = None\n",
    "        \n",
    "        return surf_features, keypoints, descriptors\n",
    "    \n",
    "    def orb_features(self, img):\n",
    "        \"\"\"Extract ORB keypoints and descriptors\"\"\"\n",
    "        orb = cv2.ORB_create(nfeatures=500)\n",
    "        keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "        \n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            orb_features = np.zeros(32)  # Fixed size\n",
    "        else:\n",
    "            descriptors_float = descriptors.astype(np.float32)\n",
    "            orb_features = np.mean(descriptors_float, axis=0)\n",
    "        \n",
    "        return orb_features, keypoints, descriptors\n",
    "    \n",
    "    def texture_features(self, img):\n",
    "        \"\"\"Extract texture features using Laplacian\"\"\"\n",
    "        log = cv2.Laplacian(img, cv2.CV_64F)\n",
    "        \n",
    "        texture_features = np.array([\n",
    "            np.mean(log),\n",
    "            np.std(log),\n",
    "            np.percentile(log, 25),\n",
    "            np.percentile(log, 75),\n",
    "        ])\n",
    "        return texture_features\n",
    "    \n",
    "    def histogram_features(self, img):\n",
    "        \"\"\"Extract histogram features\"\"\"\n",
    "        hist = cv2.calcHist([img], [0], None, [32], [0, 256])\n",
    "        hist = hist.flatten() / hist.sum()  # Normalize\n",
    "        return hist\n",
    "    \n",
    "    def extract_all_features(self, img):\n",
    "        \"\"\"Extract all features from an image\"\"\"\n",
    "        edge_feat, _ = self.edge_detection(img)\n",
    "        corner_feat, _ = self.corner_detection_harris(img)\n",
    "        blob_feat, _ = self.blob_detection(img)\n",
    "        ridge_feat, _ = self.ridge_detection(img)\n",
    "        texture_feat = self.texture_features(img)\n",
    "        \n",
    "        sift_feat, _, _ = self.sift_features(img)\n",
    "        surf_feat, _, _ = self.surf_features(img)\n",
    "        orb_feat, _, _ = self.orb_features(img)\n",
    "        \n",
    "        hist_feat = self.histogram_features(img)\n",
    "        \n",
    "        all_features = np.concatenate([\n",
    "            edge_feat, corner_feat, blob_feat, ridge_feat, texture_feat,\n",
    "            sift_feat, surf_feat, orb_feat, hist_feat\n",
    "        ])\n",
    "        \n",
    "        return all_features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38898215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.136183Z",
     "iopub.status.busy": "2025-11-09T11:43:04.135289Z",
     "iopub.status.idle": "2025-11-09T11:43:04.146977Z",
     "shell.execute_reply": "2025-11-09T11:43:04.146119Z"
    },
    "papermill": {
     "duration": 0.017048,
     "end_time": "2025-11-09T11:43:04.148298",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.131250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 3: DEEP LEARNING AUTOENCODER\n",
    "\n",
    "class AutoencoderAnomalyDetector:\n",
    "    \"\"\"Autoencoder for Anomaly Detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dim=32, contamination=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.contamination = contamination\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.threshold = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        if not KERAS_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow/Keras not installed\")\n",
    "        \n",
    "        # Encoder\n",
    "        input_layer = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        encoded = layers.Dense(self.encoding_dim, activation='relu')(encoded)\n",
    "        \n",
    "        # Decoder\n",
    "        decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        decoded = layers.Dense(self.input_dim, activation='sigmoid')(decoded)\n",
    "        \n",
    "        # Autoencoder model\n",
    "        self.model = models.Model(input_layer, decoded)\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=losses.MeanSquaredError()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1):\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Normalize data\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = self.model.fit(\n",
    "            X_train_scaled, X_train_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Set threshold based on training data\n",
    "        train_predictions = self.model.predict(X_train_scaled, verbose=0)\n",
    "        train_mse = np.mean((X_train_scaled - train_predictions) ** 2, axis=1)\n",
    "        self.threshold = np.percentile(train_mse, (1 - self.contamination) * 100)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            predictions: -1 (anomaly) or 1 (normal)\n",
    "            reconstruction_errors: MSE for each sample\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Normalize\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Get reconstructions\n",
    "        reconstructions = self.model.predict(X_test_scaled, verbose=0)\n",
    "        \n",
    "        # Compute reconstruction error (MSE)\n",
    "        reconstruction_errors = np.mean(\n",
    "            (X_test_scaled - reconstructions) ** 2,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Classify as anomaly if error > threshold\n",
    "        predictions = np.where(\n",
    "            reconstruction_errors > self.threshold,\n",
    "            -1,  # anomaly\n",
    "            1    # normal\n",
    "        )\n",
    "        \n",
    "        return predictions, reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54d3dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.155796Z",
     "iopub.status.busy": "2025-11-09T11:43:04.155475Z",
     "iopub.status.idle": "2025-11-09T11:43:04.174955Z",
     "shell.execute_reply": "2025-11-09T11:43:04.174024Z"
    },
    "papermill": {
     "duration": 0.025115,
     "end_time": "2025-11-09T11:43:04.176409",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.151294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 4: ANOMALY DETECTION METHODS\n",
    "\n",
    "class AnomalyDetectionMethods:\n",
    "    \"\"\"Comprehensive anomaly detection using multiple algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def knn_anomaly_detection(self, X_train, X_test, n_neighbors=5, percentile=95):\n",
    "        \"\"\"Detect anomalies using K-Nearest Neighbors\"\"\"\n",
    "        model = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        model.fit(X_train)\n",
    "        distances, _ = model.kneighbors(X_test)\n",
    "        anomaly_scores = distances[:, -1]\n",
    "        threshold = np.percentile(distances[:, -1], percentile)\n",
    "        predictions = (anomaly_scores > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        return model, predictions, anomaly_scores\n",
    "    \n",
    "    def pca_anomaly_detection(self, X_train, X_test, variance_explained=0.95, percentile=95):\n",
    "        \"\"\"Detect anomalies using PCA reconstruction error\"\"\"\n",
    "        pca = PCA(n_components=variance_explained)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        X_train_reconstructed = pca.inverse_transform(X_train_pca)\n",
    "        X_test_reconstructed = pca.inverse_transform(X_test_pca)\n",
    "        \n",
    "        train_errors = np.mean((X_train - X_train_reconstructed) ** 2, axis=1)\n",
    "        test_errors = np.mean((X_test - X_test_reconstructed) ** 2, axis=1)\n",
    "        \n",
    "        threshold = np.percentile(train_errors, percentile)\n",
    "        predictions = (test_errors > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        \n",
    "        return pca, predictions, test_errors\n",
    "    \n",
    "    def isolation_forest_detection(self, X_train, X_test, contamination=0.1):\n",
    "        \"\"\"Detect anomalies using Isolation Forest\"\"\"\n",
    "        model = IsolationForest(contamination=contamination, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.score_samples(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def local_outlier_factor_detection(self, X_train, X_test, n_neighbors=20, contamination=0.1, novelty=True):\n",
    "        \"\"\"Detect anomalies using Local Outlier Factor\"\"\"\n",
    "        model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=novelty)\n",
    "        X_combined = np.vstack([X_train, X_test])\n",
    "        model.fit(X_combined)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.negative_outlier_factor_\n",
    "        return model, predictions, scores[-len(X_test):]\n",
    "    \n",
    "    def one_class_svm_detection(self, X_train, X_test, nu=0.1):\n",
    "        \"\"\"Detect anomalies using One-Class SVM\"\"\"\n",
    "        model = OneClassSVM(kernel='rbf', nu=nu)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.decision_function(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def elliptic_envelope_detection(self, X_train, X_test, contamination=0.1):\n",
    "        \"\"\"Detect anomalies using Elliptic Envelope\"\"\"\n",
    "        model = EllipticEnvelope(contamination=contamination, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.decision_function(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def dbscan_anomaly_detection(self, X_train, X_test, eps=0.5, min_samples=5):\n",
    "        \"\"\"Detect anomalies using DBSCAN\"\"\"\n",
    "        X_combined = np.vstack([X_train, X_test])\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X_combined)\n",
    "        predictions = labels[-len(X_test):]\n",
    "        predictions = np.where(predictions == -1, -1, 1)\n",
    "        return model, predictions, labels[-len(X_test):]\n",
    "    \n",
    "    def gaussian_mixture_anomaly_detection(self, X_train, X_test, n_components=10, percentile=95):\n",
    "        \"\"\"Detect anomalies using Gaussian Mixture Model\"\"\"\n",
    "        model = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        train_scores = -model.score_samples(X_train)\n",
    "        test_scores = -model.score_samples(X_test)\n",
    "        threshold = np.percentile(train_scores, percentile)\n",
    "        predictions = (test_scores > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        return model, predictions, test_scores\n",
    "    \n",
    "    def kmeans_anomaly_detection(self, X_train, X_test, n_clusters=5, contamination=0.1):\n",
    "        \"\"\"Detect anomalies using K-Means\"\"\"\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "        model.fit(X_train_scaled)\n",
    "        \n",
    "        train_labels = model.predict(X_train_scaled)\n",
    "        train_distances = np.sqrt(np.sum((X_train_scaled - model.cluster_centers_[train_labels])**2, axis=1))\n",
    "        \n",
    "        threshold = np.percentile(train_distances, (1 - contamination) * 100)\n",
    "        \n",
    "        test_labels = model.predict(X_test_scaled)\n",
    "        test_distances = np.sqrt(np.sum((X_test_scaled - model.cluster_centers_[test_labels])**2, axis=1))\n",
    "        \n",
    "        predictions = np.where(test_distances > threshold, -1, 1)\n",
    "        return model, predictions, test_distances\n",
    "    \n",
    "    def adaboost_detection(self, X_train, y_train, X_test,\n",
    "                          n_estimators=100, learning_rate=0.5):\n",
    "        \"\"\"Detect anomalies using AdaBoost\"\"\"\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Create and train model\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "        model = AdaBoostClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        predictions = np.where(probabilities > 0.5, -1, 1)\n",
    "        \n",
    "        return model, predictions, probabilities\n",
    "    \n",
    "    def autoencoder_detection(self, X_train, X_test, encoding_dim=32,\n",
    "                             epochs=50, batch_size=32,\n",
    "                             contamination=0.1):\n",
    "        \"\"\"Detect anomalies using Autoencoder\"\"\"\n",
    "        input_dim = X_train.shape[1]\n",
    "        \n",
    "        # Create detector\n",
    "        detector = AutoencoderAnomalyDetector(\n",
    "            input_dim=input_dim,\n",
    "            encoding_dim=encoding_dim,\n",
    "            contamination=contamination\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        detector.fit(X_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions, reconstruction_errors = detector.predict(X_test)\n",
    "        return detector, predictions, reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f7e722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.184417Z",
     "iopub.status.busy": "2025-11-09T11:43:04.184112Z",
     "iopub.status.idle": "2025-11-09T11:43:04.212303Z",
     "shell.execute_reply": "2025-11-09T11:43:04.211359Z"
    },
    "papermill": {
     "duration": 0.034277,
     "end_time": "2025-11-09T11:43:04.213815",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.179538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 5: MAIN PIPELINE\n",
    "\n",
    "class BrainMRIAnomalyDetectionPipeline:\n",
    "    \"\"\"Complete pipeline for Brain MRI anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.loader = BrainMRIDatasetLoader(dataset_path)\n",
    "        self.extractor = ImageFeatureExtractor()\n",
    "        self.anomaly_detector = AnomalyDetectionMethods()\n",
    "        self.results_df = None\n",
    "    \n",
    "    def load_and_prepare_dataset(self, train_ratio=0.7, limit_no_tumor=None):\n",
    "        \"\"\"Load brain MRI dataset and prepare for training\"\"\"\n",
    "        print(\"Loading brain MRI dataset...\")\n",
    "        \n",
    "        # Get dataset summary\n",
    "        summary = self.loader.get_dataset_summary()\n",
    "        \n",
    "        # Load tumor images (anomalies)\n",
    "        tumor_images, tumor_paths = self.loader.load_tumor_images()\n",
    "        tumor_labels = np.ones(len(tumor_images))  # 1 for tumor\n",
    "        \n",
    "        # Load non-tumor images (normal)\n",
    "        no_tumor_images, no_tumor_paths = self.loader.load_no_tumor_images()\n",
    "        no_tumor_labels = np.zeros(len(no_tumor_images))  # 0 for normal\n",
    "        \n",
    "        # Limit non-tumor if needed\n",
    "        if limit_no_tumor and len(no_tumor_images) > limit_no_tumor:\n",
    "            indices = np.random.choice(len(no_tumor_images), limit_no_tumor, replace=False)\n",
    "            no_tumor_images = [no_tumor_images[i] for i in indices]\n",
    "            no_tumor_paths = [no_tumor_paths[i] for i in indices]\n",
    "            no_tumor_labels = no_tumor_labels[indices]\n",
    "        \n",
    "        print(f\"\\nDataset loaded:\")\n",
    "        print(f\"  Tumor images: {len(tumor_images)}\")\n",
    "        print(f\"  Non-tumor images: {len(no_tumor_images)}\")\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"\\nExtracting features...\")\n",
    "        all_images = tumor_images + no_tumor_images\n",
    "        all_labels = np.concatenate([tumor_labels, no_tumor_labels])\n",
    "        all_paths = tumor_paths + no_tumor_paths\n",
    "        \n",
    "        features_list = []\n",
    "        for idx, img in enumerate(all_images):\n",
    "            if (idx + 1) % 50 == 0 or idx == 0:\n",
    "                print(f\"  Extracting: {idx + 1}/{len(all_images)}\")\n",
    "            \n",
    "            try:\n",
    "                features = self.extractor.extract_all_features(img)\n",
    "                features_list.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error extracting features from image {idx}: {e}\")\n",
    "                # Add zeros if feature extraction fails\n",
    "                features_list.append(np.zeros((1, 256)))  # Approximate feature size\n",
    "        \n",
    "        X = np.vstack(features_list)\n",
    "        y = all_labels\n",
    "        \n",
    "        # Split into train and test\n",
    "        # Training: mostly normal images\n",
    "        normal_indices = np.where(y == 0)[0]\n",
    "        tumor_indices = np.where(y == 1)[0]\n",
    "        \n",
    "        n_train_normal = int(len(normal_indices) * train_ratio)\n",
    "        train_indices = np.concatenate([\n",
    "            normal_indices[:n_train_normal],\n",
    "            np.random.choice(tumor_indices, size=min(len(tumor_indices)//3, 5), replace=False)\n",
    "        ])\n",
    "        \n",
    "        test_indices = np.concatenate([\n",
    "            normal_indices[n_train_normal:],\n",
    "            tumor_indices[len(tumor_indices)//3:]\n",
    "        ])\n",
    "        \n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        \n",
    "        # Normalize\n",
    "        self.anomaly_detector.scaler.fit(X_train)\n",
    "        X_train = self.anomaly_detector.scaler.transform(X_train)\n",
    "        X_test = self.anomaly_detector.scaler.transform(X_test)\n",
    "        \n",
    "        print(f\"\\nDataset prepared:\")\n",
    "        print(f\"  Training: {len(X_train)} samples\")\n",
    "        print(f\"  Testing: {len(X_test)} samples\")\n",
    "        print(f\"  Features: {X.shape[1]}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, all_paths\n",
    "    \n",
    "    def run_all_methods(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Run all anomaly detection methods\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Running Anomaly Detection Methods\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'Method': [],\n",
    "            'Precision': [],\n",
    "            'Recall': [],\n",
    "            'F1-Score': [],\n",
    "            'ROC-AUC': []\n",
    "        }\n",
    "        \n",
    "        methods = [\n",
    "            ('K-Nearest Neighbors', self._run_knn),\n",
    "            ('PCA', self._run_pca),\n",
    "            ('Isolation Forest', self._run_isolation_forest),\n",
    "            ('Local Outlier Factor', self._run_lof),\n",
    "            ('One-Class SVM', self._run_ocsvm),\n",
    "            ('Elliptic Envelope', self._run_elliptic),\n",
    "            ('DBSCAN', self._run_dbscan),\n",
    "            ('Gaussian Mixture Model', self._run_gmm),\n",
    "            ('K-Means', self._run_kmeans),\n",
    "        ]\n",
    "        \n",
    "        for method_name, method_func in methods:\n",
    "            print(f\"\\n{method_name}...\")\n",
    "            try:\n",
    "                metrics = method_func(X_train, X_test, y_test)\n",
    "                self._print_results(method_name, metrics)\n",
    "                results = self._append_results(results, method_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "        \n",
    "        # Deep learning method\n",
    "        if KERAS_AVAILABLE:\n",
    "            methods_dl = [('Autoencoder', self._run_autoencoder)]\n",
    "            for method_name, method_func in methods_dl:\n",
    "                print(f\"\\n{method_name}...\")\n",
    "                try:\n",
    "                    metrics = method_func(X_train, X_test, y_test)\n",
    "                    self._print_results(method_name, metrics)\n",
    "                    results = self._append_results(results, method_name, metrics)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {e}\")\n",
    "        \n",
    "        # Supervised method\n",
    "        methods_supervised = [('AdaBoost', self._run_adaboost)]\n",
    "        for method_name, method_func in methods_supervised:\n",
    "            print(f\"\\n{method_name}...\")\n",
    "            try:\n",
    "                metrics = method_func(X_train, y_train, X_test, y_test)\n",
    "                self._print_results(method_name, metrics)\n",
    "                results = self._append_results(results, method_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "        \n",
    "        self.results_df = pd.DataFrame(results)\n",
    "        return self.results_df\n",
    "    \n",
    "    def _run_knn(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run KNN method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.knn_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_pca(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run PCA method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.pca_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_isolation_forest(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Isolation Forest method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.isolation_forest_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_lof(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run LOF method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.local_outlier_factor_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_ocsvm(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run One-Class SVM method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.one_class_svm_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_elliptic(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Elliptic Envelope method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.elliptic_envelope_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_dbscan(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run DBSCAN method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.dbscan_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_gmm(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Gaussian Mixture Model method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.gaussian_mixture_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_kmeans(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run K-Means method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.kmeans_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_autoencoder(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Autoencoder method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.autoencoder_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_adaboost(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Run AdaBoost method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.adaboost_detection(X_train, y_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _compute_metrics(self, y_true, predictions, scores):\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        y_pred_binary = np.where(predictions == -1, 1, 0)\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, scores)\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "    \n",
    "    def _print_results(self, method_name, metrics):\n",
    "        \"\"\"Print results for a method\"\"\"\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    def _append_results(self, results, method_name, metrics):\n",
    "        \"\"\"Append results to dictionary\"\"\"\n",
    "        results['Method'].append(method_name)\n",
    "        results['Precision'].append(metrics['precision'])\n",
    "        results['Recall'].append(metrics['recall'])\n",
    "        results['F1-Score'].append(metrics['f1'])\n",
    "        results['ROC-AUC'].append(metrics['roc_auc'])\n",
    "        return results\n",
    "    \n",
    "    def save_results(self, output_file='brain_mri_anomaly_results.csv'):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        if self.results_df is not None:\n",
    "            self.results_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nResults saved to {output_file}\")\n",
    "            print(\"\\nResults Summary:\")\n",
    "            print(self.results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a031a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T11:43:04.221105Z",
     "iopub.status.busy": "2025-11-09T11:43:04.220803Z",
     "iopub.status.idle": "2025-11-09T11:43:27.054680Z",
     "shell.execute_reply": "2025-11-09T11:43:27.053374Z"
    },
    "papermill": {
     "duration": 22.839368,
     "end_time": "2025-11-09T11:43:27.056201",
     "exception": false,
     "start_time": "2025-11-09T11:43:04.216833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset structure validated\n",
      "  Tumor images: /kaggle/input/brain-mri-images-for-brain-tumor-detection/yes\n",
      "  No tumor images: /kaggle/input/brain-mri-images-for-brain-tumor-detection/no\n",
      "Loading brain MRI dataset...\n",
      "\n",
      "======================================================================\n",
      "BRAIN MRI DATASET SUMMARY\n",
      "======================================================================\n",
      "Tumor images: 87\n",
      "Non-tumor images: 92\n",
      "Total images: 179\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading tumor images...\n",
      "  Loading: 1/87\n",
      "  Loading: 50/87\n",
      "Loaded 87 tumor images\n",
      "\n",
      "Loading non-tumor images...\n",
      "  Loading: 1/92\n",
      "  Loading: 50/92\n",
      "Loaded 92 non-tumor images\n",
      "\n",
      "Dataset loaded:\n",
      "  Tumor images: 87\n",
      "  Non-tumor images: 92\n",
      "\n",
      "Extracting features...\n",
      "  Extracting: 1/179\n",
      "  Extracting: 50/179\n",
      "  Extracting: 100/179\n",
      "  Extracting: 150/179\n",
      "\n",
      "Dataset prepared:\n",
      "  Training: 69 samples\n",
      "  Testing: 86 samples\n",
      "  Features: 273\n",
      "\n",
      "======================================================================\n",
      "Running Anomaly Detection Methods\n",
      "======================================================================\n",
      "\n",
      "K-Nearest Neighbors...\n",
      "  Precision: 0.4000\n",
      "  Recall: 0.0345\n",
      "  F1-Score: 0.0635\n",
      "  ROC-AUC: 0.5308\n",
      "\n",
      "PCA...\n",
      "  Precision: 0.6842\n",
      "  Recall: 0.8966\n",
      "  F1-Score: 0.7761\n",
      "  ROC-AUC: 0.4698\n",
      "\n",
      "Isolation Forest...\n",
      "  Precision: 0.6000\n",
      "  Recall: 0.1552\n",
      "  F1-Score: 0.2466\n",
      "  ROC-AUC: 0.6367\n",
      "\n",
      "Local Outlier Factor...\n",
      "  Precision: 0.3636\n",
      "  Recall: 0.0690\n",
      "  F1-Score: 0.1159\n",
      "  ROC-AUC: 0.5868\n",
      "\n",
      "One-Class SVM...\n",
      "  Precision: 0.5625\n",
      "  Recall: 0.4655\n",
      "  F1-Score: 0.5094\n",
      "  ROC-AUC: 0.7192\n",
      "\n",
      "Elliptic Envelope...\n",
      "  Precision: 0.6806\n",
      "  Recall: 0.8448\n",
      "  F1-Score: 0.7538\n",
      "  ROC-AUC: 0.5265\n",
      "\n",
      "DBSCAN...\n",
      "  Precision: 0.6585\n",
      "  Recall: 0.9310\n",
      "  F1-Score: 0.7714\n",
      "  ROC-AUC: 0.5345\n",
      "\n",
      "Gaussian Mixture Model...\n",
      "  Precision: 0.6753\n",
      "  Recall: 0.8966\n",
      "  F1-Score: 0.7704\n",
      "  ROC-AUC: 0.5283\n",
      "\n",
      "K-Means...\n",
      "  Precision: 0.7222\n",
      "  Recall: 0.4483\n",
      "  F1-Score: 0.5532\n",
      "  ROC-AUC: 0.5031\n",
      "\n",
      "Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 11:43:19.147920: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.6538\n",
      "  Recall: 0.2931\n",
      "  F1-Score: 0.4048\n",
      "  ROC-AUC: 0.3571\n",
      "\n",
      "AdaBoost...\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.1724\n",
      "  F1-Score: 0.2941\n",
      "  ROC-AUC: 0.7937\n",
      "\n",
      "Results saved to brain_mri_anomaly_results.csv\n",
      "\n",
      "Results Summary:\n",
      "                Method  Precision   Recall  F1-Score  ROC-AUC\n",
      "   K-Nearest Neighbors   0.400000 0.034483  0.063492 0.530788\n",
      "                   PCA   0.684211 0.896552  0.776119 0.469828\n",
      "      Isolation Forest   0.600000 0.155172  0.246575 0.636700\n",
      "  Local Outlier Factor   0.363636 0.068966  0.115942 0.586823\n",
      "         One-Class SVM   0.562500 0.465517  0.509434 0.719212\n",
      "     Elliptic Envelope   0.680556 0.844828  0.753846 0.526478\n",
      "                DBSCAN   0.658537 0.931034  0.771429 0.534483\n",
      "Gaussian Mixture Model   0.675325 0.896552  0.770370 0.528325\n",
      "               K-Means   0.722222 0.448276  0.553191 0.503079\n",
      "           Autoencoder   0.653846 0.293103  0.404762 0.357143\n",
      "              AdaBoost   1.000000 0.172414  0.294118 0.793719\n",
      "\n",
      "ANALYSIS COMPLETE\n",
      "Best method: PCA\n",
      "Best F1-Score: 0.7761\n",
      "Best ROC-AUC: 0.4698\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    DATASET_ROOT = \"/kaggle/input/brain-mri-images-for-brain-tumor-detection\"\n",
    "    LIMIT_NO_TUMOR = None  # Set to a number to limit non-tumor images\n",
    "    \n",
    "    try:\n",
    "        pipeline = BrainMRIAnomalyDetectionPipeline(DATASET_ROOT)\n",
    "        \n",
    "        # Load and prepare dataset\n",
    "        X_train, X_test, y_train, y_test, paths = pipeline.load_and_prepare_dataset(\n",
    "            train_ratio=0.7,\n",
    "            limit_no_tumor=LIMIT_NO_TUMOR\n",
    "        )\n",
    "        \n",
    "        # Run all anomaly detection methods\n",
    "        results = pipeline.run_all_methods(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Save results\n",
    "        pipeline.save_results(\"brain_mri_anomaly_results.csv\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nANALYSIS COMPLETE\")\n",
    "        best_idx = results['F1-Score'].idxmax()\n",
    "        print(f\"Best method: {results.loc[best_idx, 'Method']}\")\n",
    "        print(f\"Best F1-Score: {results.loc[best_idx, 'F1-Score']:.4f}\")\n",
    "        print(f\"Best ROC-AUC: {results.loc[best_idx, 'ROC-AUC']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 165566,
     "sourceId": 377107,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.368388,
   "end_time": "2025-11-09T11:43:29.757101",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T11:42:27.388713",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
