{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba0fbe6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:05.140409Z",
     "iopub.status.busy": "2025-11-09T10:15:05.140005Z",
     "iopub.status.idle": "2025-11-09T10:15:34.461371Z",
     "shell.execute_reply": "2025-11-09T10:15:34.460244Z"
    },
    "papermill": {
     "duration": 29.328523,
     "end_time": "2025-11-09T10:15:34.463175",
     "exception": false,
     "start_time": "2025-11-09T10:15:05.134652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 10:15:11.299286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762683311.530505      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762683311.599908      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest, AdaBoostClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, roc_auc_score, roc_curve)\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras import layers, models, losses, optimizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    KERAS_AVAILABLE = True\n",
    "except:\n",
    "    print(\"TensorFlow/Keras not available. Deep learning methods will be skipped.\")\n",
    "    KERAS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4783fca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.472201Z",
     "iopub.status.busy": "2025-11-09T10:15:34.471647Z",
     "iopub.status.idle": "2025-11-09T10:15:34.490994Z",
     "shell.execute_reply": "2025-11-09T10:15:34.489838Z"
    },
    "papermill": {
     "duration": 0.025573,
     "end_time": "2025-11-09T10:15:34.492644",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.467071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 1: AITEX DATASET LOADER\n",
    "\n",
    "class AITEXDatasetLoader:\n",
    "    \"\"\"Load AITEX fabric dataset with specific directory structure\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_root_path):\n",
    "        \"\"\"\n",
    "        Initialize loader for AITEX dataset\n",
    "        \"\"\"\n",
    "        self.dataset_root = Path(dataset_root_path)\n",
    "        self.defect_dir = self.dataset_root / \"Defect_images\"\n",
    "        self.no_defect_dir = self.dataset_root / \"NODefect_images\"\n",
    "        self.mask_dir = self.dataset_root / \"Mask_images\"\n",
    "        \n",
    "        self._validate_structure()\n",
    "    \n",
    "    def _validate_structure(self):\n",
    "        \"\"\"Validate that required directories exist\"\"\"\n",
    "        required_dirs = [self.defect_dir, self.no_defect_dir]\n",
    "        \n",
    "        for dir_path in required_dirs:\n",
    "            if not dir_path.exists():\n",
    "                raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "        \n",
    "        print(f\"✓ Dataset structure validated\")\n",
    "        print(f\"  Defect images: {self.defect_dir}\")\n",
    "        print(f\"  NODefect images: {self.no_defect_dir}\")\n",
    "        if self.mask_dir.exists():\n",
    "            print(f\"  Mask images: {self.mask_dir}\")\n",
    "    \n",
    "    def load_defect_images(self):\n",
    "        \"\"\"Load all defect images\"\"\"\n",
    "        print(\"\\nLoading defect images...\")\n",
    "        images = []\n",
    "        paths = []\n",
    "        \n",
    "        image_files = sorted(list(self.defect_dir.glob(\"*.png\")))\n",
    "        \n",
    "        for idx, image_file in enumerate(image_files):\n",
    "            if (idx + 1) % 50 == 0 or idx == 0:\n",
    "                print(f\"  Loading: {idx + 1}/{len(image_files)}\")\n",
    "            \n",
    "            try:\n",
    "                img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (256, 256))\n",
    "                    images.append(img)\n",
    "                    paths.append(str(image_file))\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {image_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(images)} defect images\")\n",
    "        return images, paths\n",
    "    \n",
    "    def load_no_defect_images(self):\n",
    "        \"\"\"Load all non-defect images from subcategories\"\"\"\n",
    "        print(\"\\nLoading non-defect images...\")\n",
    "        images = []\n",
    "        paths = []\n",
    "        \n",
    "        # Get all subdirectories (fabric types)\n",
    "        subdirs = sorted([d for d in self.no_defect_dir.iterdir() if d.is_dir()])\n",
    "        print(f\"Found {len(subdirs)} fabric types\")\n",
    "        \n",
    "        total_images = sum(len(list(d.glob(\"*.png\"))) for d in subdirs)\n",
    "        loaded = 0\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            fabric_type = subdir.name\n",
    "            image_files = sorted(list(subdir.glob(\"*.png\")))\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                try:\n",
    "                    img = cv2.imread(str(image_file), cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (256, 256))\n",
    "                        images.append(img)\n",
    "                        paths.append(str(image_file))\n",
    "                        loaded += 1\n",
    "                        \n",
    "                        if loaded % 100 == 0:\n",
    "                            print(f\"  Loaded: {loaded}/{total_images}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {image_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(images)} non-defect images\")\n",
    "        return images, paths\n",
    "    \n",
    "    def load_masks(self):\n",
    "        \"\"\"Load mask images for defect localization\"\"\"\n",
    "        print(\"\\nLoading mask images...\")\n",
    "        masks = {}\n",
    "        \n",
    "        if not self.mask_dir.exists():\n",
    "            print(\"Mask directory not found. Skipping.\")\n",
    "            return masks\n",
    "        \n",
    "        mask_files = sorted(list(self.mask_dir.glob(\"*.png\")))\n",
    "        \n",
    "        for mask_file in mask_files:\n",
    "            try:\n",
    "                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is not None:\n",
    "                    mask = cv2.resize(mask, (256, 256))\n",
    "                    masks[mask_file.stem] = mask\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {mask_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(masks)} mask images\")\n",
    "        return masks\n",
    "    \n",
    "    def get_dataset_summary(self):\n",
    "        \"\"\"Print dataset summary\"\"\"\n",
    "        defect_count = len(list(self.defect_dir.glob(\"*.png\")))\n",
    "        \n",
    "        no_defect_count = 0\n",
    "        for subdir in self.no_defect_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                no_defect_count += len(list(subdir.glob(\"*.png\")))\n",
    "        \n",
    "        mask_count = len(list(self.mask_dir.glob(\"*.png\"))) if self.mask_dir.exists() else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"AITEX DATASET SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Defect images:     {defect_count}\")\n",
    "        print(f\"Non-defect images: {no_defect_count}\")\n",
    "        print(f\"Mask images:       {mask_count}\")\n",
    "        print(f\"Total images:      {defect_count + no_defect_count}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'defect': defect_count,\n",
    "            'no_defect': no_defect_count,\n",
    "            'masks': mask_count,\n",
    "            'total': defect_count + no_defect_count\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ad7929",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.500912Z",
     "iopub.status.busy": "2025-11-09T10:15:34.500609Z",
     "iopub.status.idle": "2025-11-09T10:15:34.517564Z",
     "shell.execute_reply": "2025-11-09T10:15:34.516538Z"
    },
    "papermill": {
     "duration": 0.023166,
     "end_time": "2025-11-09T10:15:34.519167",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.496001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 2: FEATURE EXTRACTION (from previous implementation)\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    \"\"\"Extract multiple types of features from fabric images\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=(256, 256)):\n",
    "        self.image_size = image_size\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def edge_detection(self, img):\n",
    "        \"\"\"Extract edge features using Canny edge detection\"\"\"\n",
    "        edges = cv2.Canny(img, 50, 150)\n",
    "        edge_features = [\n",
    "            np.sum(edges) / edges.size,\n",
    "            np.std(edges),\n",
    "            np.count_nonzero(edges) / edges.size,\n",
    "        ]\n",
    "        return np.array(edge_features), edges\n",
    "    \n",
    "    def corner_detection_harris(self, img):\n",
    "        \"\"\"Extract corner features using Harris corner detection\"\"\"\n",
    "        corners = cv2.cornerHarris(img, 2, 3, 0.04)\n",
    "        corners = cv2.dilate(corners, None)\n",
    "        \n",
    "        corner_features = [\n",
    "            np.sum(corners) / corners.size,\n",
    "            np.max(corners),\n",
    "            np.count_nonzero(corners > 0.01 * corners.max()) / corners.size,\n",
    "            np.std(corners),\n",
    "        ]\n",
    "        return np.array(corner_features), corners\n",
    "    \n",
    "    def blob_detection(self, img):\n",
    "        \"\"\"Extract blob features using SimpleBlobDetector\"\"\"\n",
    "        params = cv2.SimpleBlobDetector_Params()\n",
    "        params.filterByArea = True\n",
    "        params.minArea = 10\n",
    "        params.maxArea = 5000\n",
    "        params.filterByCircularity = True\n",
    "        params.minCircularity = 0.1\n",
    "        detector = cv2.SimpleBlobDetector_create(params)\n",
    "        \n",
    "        keypoints = detector.detect(img)\n",
    "        \n",
    "        blob_features = [\n",
    "            len(keypoints),\n",
    "            np.mean([kp.size for kp in keypoints]) if keypoints else 0,\n",
    "            np.std([kp.size for kp in keypoints]) if len(keypoints) > 1 else 0,\n",
    "        ]\n",
    "        return np.array(blob_features), keypoints\n",
    "    \n",
    "    def ridge_detection(self, img):\n",
    "        \"\"\"Extract ridge features using Sobel derivatives\"\"\"\n",
    "        sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        \n",
    "        ridge_strength = np.sqrt(sobelx**2 + sobely**2)\n",
    "        \n",
    "        ridge_features = [\n",
    "            np.mean(ridge_strength),\n",
    "            np.std(ridge_strength),\n",
    "            np.percentile(ridge_strength, 95),\n",
    "        ]\n",
    "        return np.array(ridge_features), ridge_strength\n",
    "    \n",
    "    def sift_features(self, img):\n",
    "        \"\"\"Extract SIFT keypoints and descriptors\"\"\"\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        \n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            sift_features = np.zeros(128)  # Fixed size: only mean features\n",
    "        else:\n",
    "            # Use only mean of descriptors for fixed-size output\n",
    "            sift_features = np.mean(descriptors, axis=0)\n",
    "        \n",
    "        return sift_features, keypoints, descriptors\n",
    "    \n",
    "    def surf_features(self, img):\n",
    "        \"\"\"Extract SURF keypoints and descriptors\"\"\"\n",
    "        try:\n",
    "            surf = cv2.xfeatures2d.SURF_create(400)\n",
    "            keypoints, descriptors = surf.detectAndCompute(img, None)\n",
    "            \n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                surf_features = np.zeros(64)  # Fixed size\n",
    "            else:\n",
    "                surf_features = np.mean(descriptors, axis=0)\n",
    "        except:\n",
    "            surf_features = np.zeros(64)\n",
    "            keypoints = []\n",
    "            descriptors = None\n",
    "        \n",
    "        return surf_features, keypoints, descriptors\n",
    "    \n",
    "    def orb_features(self, img):\n",
    "        \"\"\"Extract ORB keypoints and descriptors\"\"\"\n",
    "        orb = cv2.ORB_create(nfeatures=500)\n",
    "        keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "        \n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            orb_features = np.zeros(32)  # Fixed size\n",
    "        else:\n",
    "            descriptors_float = descriptors.astype(np.float32)\n",
    "            orb_features = np.mean(descriptors_float, axis=0)\n",
    "        \n",
    "        return orb_features, keypoints, descriptors\n",
    "    \n",
    "    def texture_features(self, img):\n",
    "        \"\"\"Extract texture features using LBP-like statistics\"\"\"\n",
    "        log = cv2.Laplacian(img, cv2.CV_64F)\n",
    "        \n",
    "        texture_features = [\n",
    "            np.mean(log),\n",
    "            np.std(log),\n",
    "            np.percentile(log, 25),\n",
    "            np.percentile(log, 75),\n",
    "        ]\n",
    "        return np.array(texture_features)\n",
    "    \n",
    "    def extract_all_features(self, img):\n",
    "        \"\"\"Extract all features from an image\"\"\"\n",
    "        edge_feat, _ = self.edge_detection(img)\n",
    "        corner_feat, _ = self.corner_detection_harris(img)\n",
    "        blob_feat, _ = self.blob_detection(img)\n",
    "        ridge_feat, _ = self.ridge_detection(img)\n",
    "        texture_feat = self.texture_features(img)\n",
    "        \n",
    "        sift_feat, _, _ = self.sift_features(img)\n",
    "        surf_feat, _, _ = self.surf_features(img)\n",
    "        orb_feat, _, _ = self.orb_features(img)\n",
    "        \n",
    "        all_features = np.concatenate([\n",
    "            edge_feat, corner_feat, blob_feat, ridge_feat, texture_feat,\n",
    "            sift_feat, surf_feat, orb_feat\n",
    "        ])\n",
    "        \n",
    "        return all_features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed3cb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.528030Z",
     "iopub.status.busy": "2025-11-09T10:15:34.527670Z",
     "iopub.status.idle": "2025-11-09T10:15:34.542137Z",
     "shell.execute_reply": "2025-11-09T10:15:34.541209Z"
    },
    "papermill": {
     "duration": 0.021274,
     "end_time": "2025-11-09T10:15:34.543944",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.522670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoencoderAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Autoencoder for Anomaly Detection\n",
    "    \n",
    "    Theory:\n",
    "    - Autoencoders learn a compressed representation of normal data\n",
    "    - Reconstruction error = ||input - reconstruction||²\n",
    "    - Anomalies have higher reconstruction error\n",
    "    - Works with unsupervised learning (no labels needed)\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer → Encoder layers → Bottleneck → Decoder layers → Output layer\n",
    "    - Compression ratio determines bottleneck size\n",
    "    - Deeper networks capture more complex patterns\n",
    "    \n",
    "    Advantages:\n",
    "    - Learns complex, non-linear patterns\n",
    "    - Unsupervised (no labels needed)\n",
    "    - Handles high-dimensional data well\n",
    "    - Can detect novel anomalies\n",
    "    \n",
    "    Disadvantages:\n",
    "    - Needs large training data (>1000 samples)\n",
    "    - Slow training and inference\n",
    "    - Risk of overfitting\n",
    "    - Hyperparameter tuning complex\n",
    "    - Requires GPU for speed\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dim: Number of input features\n",
    "    - encoding_dim: Bottleneck dimension\n",
    "    - epochs: Training epochs (50-200)\n",
    "    - batch_size: Batch size (32-128)\n",
    "    - contamination: Expected anomaly fraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dim=32, contamination=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.contamination = contamination\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.threshold = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        if not KERAS_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow/Keras not installed\")\n",
    "        \n",
    "        # Encoder\n",
    "        input_layer = layers.Input(shape=(self.input_dim,))\n",
    "        \n",
    "        encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        encoded = layers.Dropout(0.2)(encoded)\n",
    "        encoded = layers.Dense(self.encoding_dim, activation='relu')(encoded)\n",
    "        \n",
    "        # Decoder\n",
    "        decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "        decoded = layers.Dropout(0.2)(decoded)\n",
    "        decoded = layers.Dense(self.input_dim, activation='sigmoid')(decoded)\n",
    "        \n",
    "        # Autoencoder model\n",
    "        self.model = models.Model(input_layer, decoded)\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=losses.MeanSquaredError()\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1):\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Normalize data\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        history = self.model.fit(\n",
    "            X_train_scaled, X_train_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Set threshold based on training data\n",
    "        train_predictions = self.model.predict(X_train_scaled, verbose=0)\n",
    "        train_mse = np.mean((X_train_scaled - train_predictions) ** 2, axis=1)\n",
    "        self.threshold = np.percentile(train_mse, (1 - self.contamination) * 100)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            predictions: -1 (anomaly) or 1 (normal)\n",
    "            reconstruction_errors: MSE for each sample\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Normalize\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Get reconstructions\n",
    "        reconstructions = self.model.predict(X_test_scaled, verbose=0)\n",
    "        \n",
    "        # Compute reconstruction error (MSE)\n",
    "        reconstruction_errors = np.mean(\n",
    "            (X_test_scaled - reconstructions) ** 2,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Classify as anomaly if error > threshold\n",
    "        predictions = np.where(\n",
    "            reconstruction_errors > self.threshold,\n",
    "            -1,  # anomaly\n",
    "            1    # normal\n",
    "        )\n",
    "        \n",
    "        return predictions, reconstruction_errors\n",
    "    \n",
    "    def get_reconstruction_visualization(self, X_test, num_samples=5):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            originals: Original data\n",
    "            reconstructions: Reconstructed data\n",
    "        \"\"\"\n",
    "        X_test_scaled = self.scaler.transform(X_test[:num_samples])\n",
    "        reconstructions = self.model.predict(X_test_scaled, verbose=0)\n",
    "        \n",
    "        return X_test_scaled, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6616d1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.553173Z",
     "iopub.status.busy": "2025-11-09T10:15:34.552853Z",
     "iopub.status.idle": "2025-11-09T10:15:34.573502Z",
     "shell.execute_reply": "2025-11-09T10:15:34.572351Z"
    },
    "papermill": {
     "duration": 0.027673,
     "end_time": "2025-11-09T10:15:34.575103",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.547430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 3: ANOMALY DETECTION METHODS\n",
    "\n",
    "class AnomalyDetectionMethods:\n",
    "    \"\"\"Comprehensive anomaly detection using multiple algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def knn_anomaly_detection(self, X_train, X_test, n_neighbors=5, percentile=95):\n",
    "        \"\"\"Detect anomalies using K-Nearest Neighbors\"\"\"\n",
    "        model = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        model.fit(X_train)\n",
    "        distances, _ = model.kneighbors(X_test)\n",
    "        anomaly_scores = distances[:, -1]\n",
    "        threshold = np.percentile(distances[:, -1], percentile)\n",
    "        predictions = (anomaly_scores > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        return model, predictions, anomaly_scores\n",
    "    \n",
    "    def pca_anomaly_detection(self, X_train, X_test, variance_explained=0.95, percentile=95):\n",
    "        \"\"\"Detect anomalies using PCA reconstruction error\"\"\"\n",
    "        pca = PCA(n_components=variance_explained)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        X_train_reconstructed = pca.inverse_transform(X_train_pca)\n",
    "        X_test_reconstructed = pca.inverse_transform(X_test_pca)\n",
    "        \n",
    "        train_errors = np.mean((X_train - X_train_reconstructed) ** 2, axis=1)\n",
    "        test_errors = np.mean((X_test - X_test_reconstructed) ** 2, axis=1)\n",
    "        \n",
    "        threshold = np.percentile(train_errors, percentile)\n",
    "        predictions = (test_errors > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        \n",
    "        return pca, predictions, test_errors\n",
    "    \n",
    "    def isolation_forest_detection(self, X_train, X_test, contamination=0.1):\n",
    "        \"\"\"Detect anomalies using Isolation Forest\"\"\"\n",
    "        model = IsolationForest(contamination=contamination, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.score_samples(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def local_outlier_factor_detection(self, X_train, X_test, n_neighbors=20, contamination=0.1, novelty=True):\n",
    "        \"\"\"Detect anomalies using Local Outlier Factor\"\"\"\n",
    "        model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=novelty)\n",
    "        X_combined = np.vstack([X_train, X_test])\n",
    "        model.fit(X_combined)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.negative_outlier_factor_\n",
    "        return model, predictions, scores[-len(X_test):]\n",
    "    \n",
    "    def one_class_svm_detection(self, X_train, X_test, nu=0.1):\n",
    "        \"\"\"Detect anomalies using One-Class SVM\"\"\"\n",
    "        model = OneClassSVM(kernel='rbf', nu=nu)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.decision_function(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def elliptic_envelope_detection(self, X_train, X_test, contamination=0.1):\n",
    "        \"\"\"Detect anomalies using Elliptic Envelope\"\"\"\n",
    "        model = EllipticEnvelope(contamination=contamination, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores = model.decision_function(X_test)\n",
    "        return model, predictions, scores\n",
    "    \n",
    "    def dbscan_anomaly_detection(self, X_train, X_test, eps=0.5, min_samples=5):\n",
    "        \"\"\"Detect anomalies using DBSCAN\"\"\"\n",
    "        X_combined = np.vstack([X_train, X_test])\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X_combined)\n",
    "        predictions = labels[-len(X_test):]\n",
    "        predictions = np.where(predictions == -1, -1, 1)\n",
    "        return model, predictions, labels[-len(X_test):]\n",
    "    \n",
    "    def gaussian_mixture_anomaly_detection(self, X_train, X_test, n_components=10, percentile=95):\n",
    "        \"\"\"Detect anomalies using Gaussian Mixture Model\"\"\"\n",
    "        model = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        model.fit(X_train)\n",
    "        train_scores = -model.score_samples(X_train)\n",
    "        test_scores = -model.score_samples(X_test)\n",
    "        threshold = np.percentile(train_scores, percentile)\n",
    "        predictions = (test_scores > threshold).astype(int)\n",
    "        predictions = np.where(predictions == 1, -1, 1)\n",
    "        return model, predictions, test_scores\n",
    "    \n",
    "    def adaboost_detection(self, X_train, y_train, X_test,\n",
    "                           n_estimators=100, learning_rate=0.5):\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Create and train model\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "        model = AdaBoostClassifier(\n",
    "            estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        probabilities = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        predictions = np.where(probabilities > 0.5, -1, 1)\n",
    "        \n",
    "        return model, predictions, probabilities\n",
    "    \n",
    "    def autoencoder_detection(self, X_train, X_test, encoding_dim=32,\n",
    "                              epochs=50, batch_size=32,\n",
    "                              contamination=0.1):\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        # Create detector\n",
    "        detector = AutoencoderAnomalyDetector(\n",
    "            input_dim=input_dim,\n",
    "            encoding_dim=encoding_dim,\n",
    "            contamination=contamination\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        detector.fit(X_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Predict\n",
    "        predictions, reconstruction_errors = detector.predict(X_test)\n",
    "        return detector, predictions, reconstruction_errors\n",
    "    \n",
    "    def kmeans_anomaly_detection(self, X_train, X_test, n_clusters=5, contamination=0.1):\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "\n",
    "        model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "        model.fit(X_train_scaled)\n",
    "\n",
    "        train_labels = model.predict(X_train_scaled)\n",
    "        train_distances = np.sqrt(np.sum((X_train_scaled - model.cluster_centers_[train_labels])**2, axis=1))\n",
    "        \n",
    "        threshold = np.percentile(train_distances, (1 - contamination) * 100)\n",
    "\n",
    "        test_labels = model.predict(X_test_scaled)\n",
    "        test_distances = np.sqrt(np.sum((X_test_scaled - model.cluster_centers_[test_labels])**2, axis=1))\n",
    "\n",
    "        predictions = np.where(test_distances > threshold, -1, 1)\n",
    "        return model, predictions, test_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5b9a0b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.584034Z",
     "iopub.status.busy": "2025-11-09T10:15:34.583610Z",
     "iopub.status.idle": "2025-11-09T10:15:34.613046Z",
     "shell.execute_reply": "2025-11-09T10:15:34.611983Z"
    },
    "papermill": {
     "duration": 0.036111,
     "end_time": "2025-11-09T10:15:34.614783",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.578672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECTION 4: MAIN PIPELINE FOR AITEX DATASET\n",
    "\n",
    "class AITEXAnomalyDetectionPipeline:\n",
    "    \"\"\"Complete pipeline for AITEX fabric anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.loader = AITEXDatasetLoader(dataset_path)\n",
    "        self.extractor = ImageFeatureExtractor()\n",
    "        self.anomaly_detector = AnomalyDetectionMethods()\n",
    "        self.results_df = None\n",
    "    \n",
    "    def load_and_prepare_dataset(self, train_ratio=0.7, limit_no_defect=None):\n",
    "        \"\"\"Load AITEX dataset and prepare for training\"\"\"\n",
    "        print(\"Loading AITEX dataset...\")\n",
    "        \n",
    "        # Get dataset summary\n",
    "        summary = self.loader.get_dataset_summary()\n",
    "        \n",
    "        # Load defect images\n",
    "        defect_images, defect_paths = self.loader.load_defect_images()\n",
    "        defect_labels = np.ones(len(defect_images))  # 1 for defect\n",
    "        \n",
    "        # Load non-defect images\n",
    "        no_defect_images, no_defect_paths = self.loader.load_no_defect_images()\n",
    "        no_defect_labels = np.zeros(len(no_defect_images))  # 0 for normal\n",
    "        \n",
    "        # Limit non-defect if needed\n",
    "        if limit_no_defect and len(no_defect_images) > limit_no_defect:\n",
    "            indices = np.random.choice(len(no_defect_images), limit_no_defect, replace=False)\n",
    "            no_defect_images = [no_defect_images[i] for i in indices]\n",
    "            no_defect_paths = [no_defect_paths[i] for i in indices]\n",
    "            no_defect_labels = no_defect_labels[indices]\n",
    "        \n",
    "        print(f\"\\nDataset loaded:\")\n",
    "        print(f\"  Defect images: {len(defect_images)}\")\n",
    "        print(f\"  Non-defect images: {len(no_defect_images)}\")\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"\\nExtracting features...\")\n",
    "        all_images = defect_images + no_defect_images\n",
    "        all_labels = np.concatenate([defect_labels, no_defect_labels])\n",
    "        all_paths = defect_paths + no_defect_paths\n",
    "        \n",
    "        features_list = []\n",
    "        for idx, img in enumerate(all_images):\n",
    "            if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                print(f\"  Extracting: {idx + 1}/{len(all_images)}\")\n",
    "            \n",
    "            try:\n",
    "                features = self.extractor.extract_all_features(img)\n",
    "                features_list.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error extracting features from image {idx}: {e}\")\n",
    "                features_list.append(np.zeros((1, 241)))\n",
    "        \n",
    "        X = np.vstack(features_list)\n",
    "        y = all_labels\n",
    "        \n",
    "        # Split into train and test\n",
    "        # Training: mostly normal images\n",
    "        normal_indices = np.where(y == 0)[0]\n",
    "        defect_indices = np.where(y == 1)[0]\n",
    "        \n",
    "        n_train_normal = int(len(normal_indices) * train_ratio)\n",
    "        train_indices = np.concatenate([\n",
    "            normal_indices[:n_train_normal],\n",
    "            np.random.choice(defect_indices, size=min(len(defect_indices)//3, 5), replace=False)\n",
    "        ])\n",
    "        \n",
    "        test_indices = np.concatenate([\n",
    "            normal_indices[n_train_normal:],\n",
    "            defect_indices[len(defect_indices)//3:]\n",
    "        ])\n",
    "        \n",
    "        X_train = X[train_indices]\n",
    "        X_test = X[test_indices]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        \n",
    "        # Normalize\n",
    "        self.anomaly_detector.scaler.fit(X_train)\n",
    "        X_train = self.anomaly_detector.scaler.transform(X_train)\n",
    "        X_test = self.anomaly_detector.scaler.transform(X_test)\n",
    "        \n",
    "        print(f\"\\nDataset prepared:\")\n",
    "        print(f\"  Training: {len(X_train)} samples\")\n",
    "        print(f\"  Testing: {len(X_test)} samples\")\n",
    "        print(f\"  Features: {X.shape[1]}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, all_paths\n",
    "    \n",
    "    def run_all_methods(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Run all anomaly detection methods\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Running Anomaly Detection Methods\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'Method': [],\n",
    "            'Precision': [],\n",
    "            'Recall': [],\n",
    "            'F1-Score': [],\n",
    "            'ROC-AUC': []\n",
    "        }\n",
    "        \n",
    "        methods = [\n",
    "            ('K-Nearest Neighbors', self._run_knn),\n",
    "            ('PCA', self._run_pca),\n",
    "            ('Isolation Forest', self._run_isolation_forest),\n",
    "            ('Local Outlier Factor', self._run_lof),\n",
    "            ('One-Class SVM', self._run_ocsvm),\n",
    "            ('Elliptic Envelope', self._run_elliptic),\n",
    "            ('DBSCAN', self._run_dbscan),\n",
    "            ('Gaussian Mixture Model', self._run_gmm),\n",
    "            ('K-Means', self._run_kmeans),\n",
    "            ('Autoencoder', self._run_autoencoder)\n",
    "        ]\n",
    "        \n",
    "        for method_name, method_func in methods:\n",
    "            print(f\"\\n{method_name}...\")\n",
    "            try:\n",
    "                metrics = method_func(X_train, X_test, y_test)\n",
    "                self._print_results(method_name, metrics)\n",
    "                results = self._append_results(results, method_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "        \n",
    "        methods = [('AdaBoost', self._run_adaboost)]\n",
    "        for method_name, method_func in methods:\n",
    "            print(f\"\\n{method_name}...\")\n",
    "            try:\n",
    "                metrics = method_func(X_train, y_train, X_test, y_test)\n",
    "                self._print_results(method_name, metrics)\n",
    "                results = self._append_results(results, method_name, metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "        \n",
    "        self.results_df = pd.DataFrame(results)\n",
    "        return self.results_df\n",
    "    \n",
    "    def _run_knn(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run KNN method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.knn_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_pca(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run PCA method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.pca_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_isolation_forest(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Isolation Forest method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.isolation_forest_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_lof(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run LOF method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.local_outlier_factor_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_ocsvm(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run One-Class SVM method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.one_class_svm_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_elliptic(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Elliptic Envelope method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.elliptic_envelope_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_dbscan(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run DBSCAN method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.dbscan_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_gmm(self, X_train, X_test, y_test):\n",
    "        \"\"\"Run Gaussian Mixture Model method\"\"\"\n",
    "        model, predictions, scores = self.anomaly_detector.gaussian_mixture_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_kmeans(self, X_train, X_test, y_test):\n",
    "        model, predictions, scores = self.anomaly_detector.kmeans_anomaly_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_autoencoder(self, X_train, X_test, y_test):\n",
    "        model, predictions, scores = self.anomaly_detector.autoencoder_detection(X_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "    \n",
    "    def _run_adaboost(self, X_train, y_train, X_test, y_test):\n",
    "        model, predictions, scores = self.anomaly_detector.adaboost_detection(X_train, y_train, X_test)\n",
    "        return self._compute_metrics(y_test, predictions, scores)\n",
    "\n",
    "    def _compute_metrics(self, y_true, predictions, scores):\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        y_pred_binary = np.where(predictions == -1, 1, 0)\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, scores)\n",
    "        except:\n",
    "            roc_auc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "    \n",
    "    def _print_results(self, method_name, metrics):\n",
    "        \"\"\"Print results for a method\"\"\"\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    def _append_results(self, results, method_name, metrics):\n",
    "        \"\"\"Append results to dictionary\"\"\"\n",
    "        results['Method'].append(method_name)\n",
    "        results['Precision'].append(metrics['precision'])\n",
    "        results['Recall'].append(metrics['recall'])\n",
    "        results['F1-Score'].append(metrics['f1'])\n",
    "        results['ROC-AUC'].append(metrics['roc_auc'])\n",
    "        return results\n",
    "    \n",
    "    def save_results(self, output_file='aitex_anomaly_results.csv'):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        if self.results_df is not None:\n",
    "            self.results_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nResults saved to {output_file}\")\n",
    "            print(\"\\nResults Summary:\")\n",
    "            print(self.results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0d1fad",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T10:15:34.622959Z",
     "iopub.status.busy": "2025-11-09T10:15:34.622651Z",
     "iopub.status.idle": "2025-11-09T10:16:08.832860Z",
     "shell.execute_reply": "2025-11-09T10:16:08.831454Z"
    },
    "papermill": {
     "duration": 34.216211,
     "end_time": "2025-11-09T10:16:08.834460",
     "exception": false,
     "start_time": "2025-11-09T10:15:34.618249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset structure validated\n",
      "  Defect images: /kaggle/input/aitex-fabric-image-database/Defect_images\n",
      "  NODefect images: /kaggle/input/aitex-fabric-image-database/NODefect_images\n",
      "  Mask images: /kaggle/input/aitex-fabric-image-database/Mask_images\n",
      "Loading AITEX dataset...\n",
      "\n",
      "======================================================================\n",
      "AITEX DATASET SUMMARY\n",
      "======================================================================\n",
      "Defect images:     106\n",
      "Non-defect images: 141\n",
      "Mask images:       107\n",
      "Total images:      247\n",
      "======================================================================\n",
      "\n",
      "Loading defect images...\n",
      "  Loading: 1/106\n",
      "  Loading: 50/106\n",
      "  Loading: 100/106\n",
      "Loaded 106 defect images\n",
      "\n",
      "Loading non-defect images...\n",
      "Found 7 fabric types\n",
      "  Loaded: 100/141\n",
      "Loaded 141 non-defect images\n",
      "\n",
      "Dataset loaded:\n",
      "  Defect images: 106\n",
      "  Non-defect images: 141\n",
      "\n",
      "Extracting features...\n",
      "  Extracting: 1/247\n",
      "  Extracting: 100/247\n",
      "  Extracting: 200/247\n",
      "\n",
      "Dataset prepared:\n",
      "  Training: 103 samples\n",
      "  Testing: 114 samples\n",
      "  Features: 241\n",
      "\n",
      "======================================================================\n",
      "Running Anomaly Detection Methods\n",
      "======================================================================\n",
      "\n",
      "K-Nearest Neighbors...\n",
      "  Precision: 0.5000\n",
      "  Recall: 0.0423\n",
      "  F1-Score: 0.0779\n",
      "  ROC-AUC: 0.4720\n",
      "\n",
      "PCA...\n",
      "  Precision: 0.5816\n",
      "  Recall: 0.8028\n",
      "  F1-Score: 0.6746\n",
      "  ROC-AUC: 0.3790\n",
      "\n",
      "Isolation Forest...\n",
      "  Precision: 0.3462\n",
      "  Recall: 0.2535\n",
      "  F1-Score: 0.2927\n",
      "  ROC-AUC: 0.8251\n",
      "\n",
      "Local Outlier Factor...\n",
      "  Precision: 0.7778\n",
      "  Recall: 0.1972\n",
      "  F1-Score: 0.3146\n",
      "  ROC-AUC: 0.3806\n",
      "\n",
      "One-Class SVM...\n",
      "  Precision: 0.5333\n",
      "  Recall: 0.6761\n",
      "  F1-Score: 0.5963\n",
      "  ROC-AUC: 0.6826\n",
      "\n",
      "Elliptic Envelope...\n",
      "  Precision: 0.6091\n",
      "  Recall: 0.9437\n",
      "  F1-Score: 0.7403\n",
      "  ROC-AUC: 0.5863\n",
      "\n",
      "DBSCAN...\n",
      "  Precision: 0.6228\n",
      "  Recall: 1.0000\n",
      "  F1-Score: 0.7676\n",
      "  ROC-AUC: 0.5000\n",
      "\n",
      "Gaussian Mixture Model...\n",
      "  Precision: 0.6091\n",
      "  Recall: 0.9437\n",
      "  F1-Score: 0.7403\n",
      "  ROC-AUC: 0.3898\n",
      "\n",
      "K-Means...\n",
      "  Precision: 0.6296\n",
      "  Recall: 0.4789\n",
      "  F1-Score: 0.5440\n",
      "  ROC-AUC: 0.4324\n",
      "\n",
      "Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 10:16:00.751567: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Precision: 0.3051\n",
      "  Recall: 0.2535\n",
      "  F1-Score: 0.2769\n",
      "  ROC-AUC: 0.1323\n",
      "\n",
      "AdaBoost...\n",
      "  Precision: 0.9000\n",
      "  Recall: 0.1268\n",
      "  F1-Score: 0.2222\n",
      "  ROC-AUC: 0.5244\n",
      "\n",
      "Results saved to aitex_anomaly_results.csv\n",
      "\n",
      "Results Summary:\n",
      "                Method  Precision   Recall  F1-Score  ROC-AUC\n",
      "   K-Nearest Neighbors   0.500000 0.042254  0.077922 0.471995\n",
      "                   PCA   0.581633 0.802817  0.674556 0.378972\n",
      "      Isolation Forest   0.346154 0.253521  0.292683 0.825090\n",
      "  Local Outlier Factor   0.777778 0.197183  0.314607 0.380609\n",
      "         One-Class SVM   0.533333 0.676056  0.596273 0.682607\n",
      "     Elliptic Envelope   0.609091 0.943662  0.740331 0.586309\n",
      "                DBSCAN   0.622807 1.000000  0.767568 0.500000\n",
      "Gaussian Mixture Model   0.609091 0.943662  0.740331 0.389781\n",
      "               K-Means   0.629630 0.478873  0.544000 0.432362\n",
      "           Autoencoder   0.305085 0.253521  0.276923 0.132329\n",
      "              AdaBoost   0.900000 0.126761  0.222222 0.524402\n",
      "ANALYSIS COMPLETE\n",
      "Best method: DBSCAN\n",
      "Best F1-Score: 0.7676\n",
      "Best ROC-AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    DATASET_ROOT = \"/kaggle/input/aitex-fabric-image-database\"\n",
    "    LIMIT_NO_DEFECT = None  # Set to a number to limit non-defect images\n",
    "    \n",
    "    try:\n",
    "        pipeline = AITEXAnomalyDetectionPipeline(DATASET_ROOT)\n",
    "        \n",
    "        # Load and prepare dataset\n",
    "        X_train, X_test, y_train, y_test, paths = pipeline.load_and_prepare_dataset(\n",
    "            train_ratio=0.7,\n",
    "            limit_no_defect=LIMIT_NO_DEFECT\n",
    "        )\n",
    "        \n",
    "        # Run all anomaly detection methods\n",
    "        results = pipeline.run_all_methods(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Save results\n",
    "        pipeline.save_results(\"aitex_anomaly_results.csv\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        best_idx = results['F1-Score'].idxmax()\n",
    "        print(f\"Best method: {results.loc[best_idx, 'Method']}\")\n",
    "        print(f\"Best F1-Score: {results.loc[best_idx, 'F1-Score']:.4f}\")\n",
    "        print(f\"Best ROC-AUC: {results.loc[best_idx, 'ROC-AUC']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a8012",
   "metadata": {
    "papermill": {
     "duration": 0.003811,
     "end_time": "2025-11-09T10:16:08.842501",
     "exception": false,
     "start_time": "2025-11-09T10:16:08.838690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3379961,
     "sourceId": 5881289,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.321501,
   "end_time": "2025-11-09T10:16:11.620676",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T10:15:00.299175",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
